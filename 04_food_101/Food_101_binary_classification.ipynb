{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "cy3gUzelOcID",
        "0xEdkfU1tMyD",
        "0QJZi2vUuQBB",
        "9GDiScOUx9uB",
        "asOgU65fO67T",
        "G5FHCTlKP2jS"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Food 101 Challenge\n",
        "\n",
        "Author: Felipe C. de Pauli\n",
        "Date: 20/11/2023\n",
        "\n",
        "This challenge was made in four steps:\n",
        "\n",
        "1. A simple walk through the data, creating a binary classification with pizza and steak classes;\n",
        "2. Create a multiclass classification with 10 classes and adjust hyperparameters;\n",
        "3. Create the final notebook with all 101 classes and test the execution;\n",
        "4. Create the final python program to run in a computer with GPU."
      ],
      "metadata": {
        "id": "aM3e1iVpJoPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Binary Classification\n",
        "\n",
        "Let's build a convolutional neural network to find patterns in our images, more specifically we need a way to:\n",
        "\n",
        "1. Load and learn about our images\n",
        "2. Preprocess the images\n",
        "3. Build a CNN to find patterns in our images\n",
        "4. Compile our CNN\n",
        "5. Fit the CNN to our training data\n",
        "6. Evaluate the model and restart the process"
      ],
      "metadata": {
        "id": "qUQ9gK_VJt_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Z_lu-XvMK3YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look if there is a GPU available to use (if you are using the colab, you have to enable the GPU on notebook settings)."
      ],
      "metadata": {
        "id": "_Dnq7WohLV99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Available GPUs: \", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "id": "4N3MaQB6K3I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this moment, I'm using COLAB in an environment with GPU. Then, it appears over there. For now, we got a GPU!"
      ],
      "metadata": {
        "id": "2XbyiSVBLg27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load and learn about our images"
      ],
      "metadata": {
        "id": "cy3gUzelOcID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data"
      ],
      "metadata": {
        "id": "0xEdkfU1tMyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we have to get the data and prepare it.\n",
        "\n",
        "Ps.: I found another place to get the data wihtout the need to get a key."
      ],
      "metadata": {
        "id": "941ytkHjtH93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL7CSVKxtAV8"
      },
      "outputs": [],
      "source": [
        "# Getting data from ztm (a Deep Learning Course's Plataform)\n",
        "import zipfile\n",
        "\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
        "\n",
        "zip_fd = zipfile.ZipFile(\"pizza_steak.zip\")\n",
        "zip_fd.extractall()\n",
        "zip_fd.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the Data (become one with it)"
      ],
      "metadata": {
        "id": "0QJZi2vUuQBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pizza_steak"
      ],
      "metadata": {
        "id": "6yzU-QowtlCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pizza_steak/train"
      ],
      "metadata": {
        "id": "I6X-BV-nuXD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pizza_steak/test/pizza"
      ],
      "metadata": {
        "id": "ZG98ja1HuZuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the pizza_steak directory, we have all images of pizzas and steaks from Food-101. We will begin with this simpler case.\n",
        "\n",
        "We got the following directories with images\n",
        "* pizza_steak/train/pizza\n",
        "* pizza_steak/test/pizza\n",
        "* pizza_steak/train/steak\n",
        "* pizza_steak/test/steak"
      ],
      "metadata": {
        "id": "ce8BKzP-MBpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for dirpath, dirname, filename in os.walk(\"pizza_steak\"):\n",
        "    if (len(filename) == 0):\n",
        "        continue\n",
        "    if (len(dirpath) > 0):\n",
        "        print(\">>\", dirpath)\n",
        "    if (len(filename) > 0):\n",
        "        print(\"   images: \", len(filename))"
      ],
      "metadata": {
        "id": "OMIcBBBjudsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need the name of the classes used. This is very easy now, but when we are working with 101, that way to get it will be very useful."
      ],
      "metadata": {
        "id": "1x2no8jkwXfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "# Generate the Path object data_dir\n",
        "data_dir = pathlib.Path(\"pizza_steak/train\")\n",
        "\n",
        "# Get from Path object all the names os each file inside this directory\n",
        "classes = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "fdJA3uf_wXJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize our images\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "def view_random_image(target_dir, target_class):\n",
        "  # Setup the target directory (we'll view images from here)\n",
        "  target_folder = target_dir+target_class\n",
        "\n",
        "  # Get a random image path\n",
        "  random_image = random.sample(os.listdir(target_folder), 1)\n",
        "  print(random_image)\n",
        "\n",
        "  # Read in the image and plot it using matplotlib\n",
        "  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "  plt.imshow(img)\n",
        "  plt.title(target_class)\n",
        "  plt.axis(\"off\");\n",
        "\n",
        "  print(f\"Image shape: {img.shape}\") # show the shape of the image\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "T3UsXL4jwtrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a random image from the training dataset\n",
        "img = view_random_image(target_dir=\"pizza_steak/train/\",\n",
        "                        target_class=\"pizza\")"
      ],
      "metadata": {
        "id": "_We97Iq7w67s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ok. We've used the mpimg to get an image from our computer and plot it using matplotlib.\n",
        "# Matplotlib plots images using the RGB system. Then you have to give to it\n",
        "# an image of (cols, rows, channels)\n",
        "print(type(img))\n",
        "print(img.shape)\n",
        "\n",
        "# But to work with tensoflow, we need a tensor. A tensor is a structured that\n",
        "# stores descriptions of objects. A vector is a tensor, but we can have strucutures\n",
        "# more complex than a vector to describe an object, and it could be using a tensor.\n",
        "# We need to cast img as a tensor.\n",
        "tf.constant(img)"
      ],
      "metadata": {
        "id": "aFgrSxWgw9Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image is a huge tensor with 3 channel per pixel. The number of columns and rows could vary. That's not good for a neural network's input."
      ],
      "metadata": {
        "id": "hPY_CZtGxdSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "Pw7a0naCxKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocess images"
      ],
      "metadata": {
        "id": "9GDiScOUx9uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"./pizza_steak/train\"\n",
        "test_data_path  = \"./pizza_steak/test\""
      ],
      "metadata": {
        "id": "dKNP_m4vzXsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1/255.)\n",
        "test_data_gen  = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "train_data = train_data_gen.flow_from_directory(\n",
        "    directory   = train_data_path,\n",
        "    batch_size  = 32,\n",
        "    target_size = (224, 224),\n",
        "    seed        = 42,\n",
        "    class_mode  = \"binary\"\n",
        ")\n",
        "\n",
        "test_data  = test_data_gen.flow_from_directory(\n",
        "    directory   = test_data_path,\n",
        "    batch_size  = 32,\n",
        "    target_size = (224, 224),\n",
        "    seed        = 42,\n",
        "    class_mode  = \"binary\"\n",
        ")"
      ],
      "metadata": {
        "id": "5vdmWdIoyCvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Build a CNN to find patterns in our images"
      ],
      "metadata": {
        "id": "asOgU65fO67T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the train_data and the test_data ready to cnn input. Now let's create our CNN!"
      ],
      "metadata": {
        "id": "ADIyEv9y0LOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten"
      ],
      "metadata": {
        "id": "aPxyi8u72uvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the architecture\n",
        "model_1 = Sequential([\n",
        "    Conv2D(\n",
        "        filters     = 10,\n",
        "        kernel_size = (3, 3),\n",
        "        activation  = \"relu\",\n",
        "        input_shape = (224, 224, 3)\n",
        "    ),\n",
        "    Conv2D(10, 3, activation=\"relu\"),\n",
        "    MaxPool2D(\n",
        "        pool_size   = 2,\n",
        "        padding     = \"valid\"\n",
        "    ),\n",
        "\n",
        "    Conv2D(10, 3, activation=\"relu\"),\n",
        "    Conv2D(10, 3, activation=\"relu\"),\n",
        "    MaxPool2D(),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(1, activation = \"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "xUcY3OYkxqrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Compile our CNN"
      ],
      "metadata": {
        "id": "G5FHCTlKP2jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile it with important hyperparameters\n",
        "model_1.compile(\n",
        "    optimizer   = tf.keras.optimizers.Adam(),\n",
        "    metrics     = [\"accuracy\"],\n",
        "    loss        = \"binary_crossentropy\"\n",
        ")"
      ],
      "metadata": {
        "id": "ya6R8pmI3ZJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Fit the CNN to our training data"
      ],
      "metadata": {
        "id": "NLxLATkOP-U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time to train\n",
        "history_1 = model_1.fit(\n",
        "    train_data,\n",
        "    epochs           = 10,\n",
        "    steps_per_epoch  = len(train_data),\n",
        "    validation_data  = test_data,\n",
        "    validation_steps = len(test_data)\n",
        ")"
      ],
      "metadata": {
        "id": "OQOEaUAu35WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got a good accuracy (training and test sets)."
      ],
      "metadata": {
        "id": "Irh0e2QQRiLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluate the model"
      ],
      "metadata": {
        "id": "ukgqD0b2QD8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to import and image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename=None, url=None, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor and reshapes it\n",
        "  to (img_shape, img_shape, colour_channels).\n",
        "  \"\"\"\n",
        "\n",
        "  if url is not None:\n",
        "    filename = tf.keras.utils.get_file(origin=url, fname=url.split('/')[-1], cache_dir='.', cache_subdir='')\n",
        "\n",
        "  # Read in the image (instead using matplotlib, as we won't plot, let's use tensorflow method)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor\n",
        "  img = tf.image.decode_image(img)\n",
        "\n",
        "  # Resize the image (with the shape of CNN's input layer)\n",
        "  img = tf.image.resize(img, size=[img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img"
      ],
      "metadata": {
        "id": "V7Kqm9Zq5Fgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = load_and_prep_image(\"pizza_steak/test/pizza/1001116.jpg\")\n",
        "img"
      ],
      "metadata": {
        "id": "c4FNwJ-T6A3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model_1.predict(tf.expand_dims(img, axis=0))\n",
        "pred"
      ],
      "metadata": {
        "id": "ZiRodDRL4Xk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero is Pizza and One is Steak."
      ],
      "metadata": {
        "id": "zgu9aI8C6ntm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_steak_or_pizza(img_path=None, url=None):\n",
        "    img = load_and_prep_image(img_path, url)\n",
        "    pred = model_1.predict(tf.expand_dims(img, axis=0))\n",
        "    if pred > 0.5:\n",
        "        print(\"You got a steak\")\n",
        "    else:\n",
        "        print(\"You got a pizza\")"
      ],
      "metadata": {
        "id": "J8Sz8ltt6O5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_steak_or_pizza(\"pizza_steak/test/pizza/1032754.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/pizza/103708.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/pizza/1060407.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/pizza/121960.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/pizza/138961.jpg\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8u4lUfJS62dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_steak_or_pizza(\"pizza_steak/test/steak/100274.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/steak/1012080.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/steak/108310.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/steak/13023.jpg\")\n",
        "predict_steak_or_pizza(\"pizza_steak/test/steak/13719.jpg\")"
      ],
      "metadata": {
        "id": "_KB5hSNxTVSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks very good! Let's see with our own images?"
      ],
      "metadata": {
        "id": "exlKkv4xT9jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only pizzas\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSYc4FDO-ZSCqCtWfpb7AX4RBYUvWXwGd1_aFAEjoyODVmMv0syOjNFoHSy0g6j5uU7Jes&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnZMTxbiq-6Rk6w5wajajLa3eSApBkTHioMobQ54DBz_cnQliOe3OXYc_5dQof7qLZn3Q&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT4hrnTXz5Dr4aVHN6xkb3hg85Q5F4z5Nxiboi8o176skOSZjlTHh99NkaDt8e-SqznwCs&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxpXTxE5QO4Y5S6FHQYiQ6-uf1Qwe6FHb28YDggeuamrPOUsIdP2Nt1OlY6sCZgJSYFI&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://media-cdn.tripadvisor.com/media/photo-s/17/98/96/31/photo0jpg.jpg\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyo8-Fg7Pa_8RQdKuLrWY6A5MQDIQgQxPuVTzA4Po8On3rMWl9I9NOY24WLIpHMOqUyss&usqp=CAU\")"
      ],
      "metadata": {
        "id": "Bgg4LlQe7X2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well... it seems this is not as good as could be. We got 3 right predictions, and 3 mistakes... 50%! Maybe our model is not generalizing very well."
      ],
      "metadata": {
        "id": "tsXJd8R1WdLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification, a new begin"
      ],
      "metadata": {
        "id": "BioOxMS4_Z3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get the data (we have already gotten it)\n",
        "2. Inspect the data (visualize, visualize, visualize - become one with it)\n",
        "3. Preprocess de data (prepare it for our model)\n",
        "4. Create a model\n",
        "5. Fit the model\n",
        "6. Evaluate the model\n",
        "7. Adjust different parameters and improve the model\n",
        "8. Repeat until satisfied"
      ],
      "metadata": {
        "id": "tkn3WmfF_i1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Become one with the data"
      ],
      "metadata": {
        "id": "944AwmyG_7Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already have a function to show the images:\n",
        "\n",
        "view_random_image\n",
        "\n",
        "as we have two classes, let's show the both on one unique figure."
      ],
      "metadata": {
        "id": "N53aAsFd_-iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "dump = view_random_image(\"pizza_steak/train/\", \"steak\")\n",
        "plt.subplot(1, 2, 2)\n",
        "dump = view_random_image(\"pizza_steak/train/\", \"pizza\")"
      ],
      "metadata": {
        "id": "i9ORfiqP8S3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocess the data (prepare it for a model)"
      ],
      "metadata": {
        "id": "GYC40zAgA2sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1/255.)\n",
        "test_data_gen  = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "train_data = train_data_gen.flow_from_directory(\n",
        "    directory       = train_data_path,\n",
        "    batch_size      = 32,\n",
        "    target_size     = (224, 224),\n",
        "    seed            = 42,\n",
        "    class_mode      = \"binary\"\n",
        ")\n",
        "\n",
        "test_data  = test_data_gen.flow_from_directory(\n",
        "    directory       = test_data_path,\n",
        "    batch_size      = 1,\n",
        "    target_size     = (224, 224),\n",
        "    seed            = 42,\n",
        "    class_mode      = \"binary\"\n",
        ")"
      ],
      "metadata": {
        "id": "NGzbGyAgAdt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_data and test_data are generators. Then, to get a batch of images (32 for training and 1 for testing) we use next()."
      ],
      "metadata": {
        "id": "nLjWawHPCGSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a CNN model (start with a baseline)"
      ],
      "metadata": {
        "id": "Z-Kd727ECimW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = Sequential([\n",
        "    Conv2D(\n",
        "        filters     = 10,\n",
        "        kernel_size = (3,3),\n",
        "        strides     = (1, 1),\n",
        "        padding     = \"valid\",\n",
        "        input_shape = (224, 224, 3),\n",
        "        activation  = \"relu\"\n",
        "    ),\n",
        "    Conv2D(10, 3, 1, padding=\"valid\", activation=\"relu\"),\n",
        "    MaxPool2D(),\n",
        "\n",
        "    Conv2D(10, 3, 1, padding=\"valid\", activation=\"relu\"),\n",
        "    Conv2D(10, 3, 1, padding=\"valid\", activation=\"relu\"),\n",
        "    MaxPool2D(),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "6RJwrJkdCh1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    metrics = [\"accuracy\"],\n",
        "    loss = \"binary_crossentropy\"\n",
        ")"
      ],
      "metadata": {
        "id": "3OSgNrTHBHzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "0ckyIxfnEWzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = model_2.fit(\n",
        "    train_data,\n",
        "    epochs           = 10,\n",
        "    steps_per_epoch  = len(train_data),\n",
        "    validation_data  = test_data,\n",
        "    validation_steps = len(test_data)\n",
        ")"
      ],
      "metadata": {
        "id": "O4Sw18UfD8vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Evaluating our model"
      ],
      "metadata": {
        "id": "2k8I5k95F6Bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(history_2.history))\n",
        "print(history_2.history.keys())"
      ],
      "metadata": {
        "id": "gf8gDmvxGN3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(history_2.history).plot(figsize=(10,7))"
      ],
      "metadata": {
        "id": "uufzxFDkEJ2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation and training curves separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\"\n",
        "  loss = history.history[\"loss\"]\n",
        "  val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "  accuracy = history.history[\"accuracy\"]\n",
        "  val_accuracy = history.history[\"val_accuracy\"]\n",
        "\n",
        "  epochs = range(len(history.history[\"loss\"])) # how many epochs did we run for?\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label=\"training_loss\")\n",
        "  plt.plot(epochs, val_loss, label=\"val_loss\")\n",
        "  plt.title(\"loss\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label=\"training_accuracy\")\n",
        "  plt.plot(epochs, val_accuracy, label=\"val_accuracy\")\n",
        "  plt.title(\"accuracy\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.legend();"
      ],
      "metadata": {
        "id": "UZj1P8yqGGx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_2)"
      ],
      "metadata": {
        "id": "JrQ4YmgHGxe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems we have overfitting whilst the epochs increase. We can use data augmentation to try breaking that behavior."
      ],
      "metadata": {
        "id": "oBz-wm3cHk8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen_augmented = ImageDataGenerator(\n",
        "    rescale             = 1/255.,\n",
        "    rotation_range      = 0.2,\n",
        "    shear_range         = 0.2,\n",
        "    zoom_range          = 0.2,\n",
        "    width_shift_range   = 0.2,\n",
        "    height_shift_range  = 0.2,\n",
        "    horizontal_flip     = True\n",
        ")\n",
        "\n",
        "train_data_augmented = train_datagen_augmented.flow_from_directory(\n",
        "    directory       = test_data_path,\n",
        "    batch_size      = 16,\n",
        "    target_size     = (224, 224),\n",
        "    seed            = 42,\n",
        "    class_mode      = \"binary\",\n",
        ")"
      ],
      "metadata": {
        "id": "afG-jRX_GzbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = model_2.fit(\n",
        "    train_data_augmented,\n",
        "    epochs              = 10,\n",
        "    steps_per_epoch     = len(train_data_augmented),\n",
        "    validation_data     = test_data,\n",
        "    validation_steps    = len(test_data)\n",
        ")"
      ],
      "metadata": {
        "id": "RiZVi18qIXZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_2)"
      ],
      "metadata": {
        "id": "om1_NNRcJKiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to be better. Let's try to use the shuffle argument."
      ],
      "metadata": {
        "id": "H_Lpv9zEJqRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_augmented = train_datagen_augmented.flow_from_directory(\n",
        "    directory       = test_data_path,\n",
        "    batch_size      = 32,\n",
        "    target_size     = (224, 224),\n",
        "    seed            = 42,\n",
        "    class_mode      = \"binary\",\n",
        "    shuffle         = True\n",
        ")"
      ],
      "metadata": {
        "id": "KvINgQQtJ65S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = model_2.fit(\n",
        "    train_data_augmented,\n",
        "    epochs              = 10,\n",
        "    steps_per_epoch     = len(train_data_augmented),\n",
        "    validation_data     = test_data,\n",
        "    validation_steps    = len(test_data)\n",
        ")"
      ],
      "metadata": {
        "id": "ANk7a7JjKBXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_2)"
      ],
      "metadata": {
        "id": "YVLUOCszKVxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_steak_or_pizza(img_path=None, url=None):\n",
        "    img = load_and_prep_image(img_path, url)\n",
        "    pred = model_2.predict(tf.expand_dims(img, axis=0))\n",
        "    if pred > 0.5:\n",
        "        print(\"You got a steak\")\n",
        "    else:\n",
        "        print(\"You got a pizza\")"
      ],
      "metadata": {
        "id": "HbIdFtUhYcha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only pizzas\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSYc4FDO-ZSCqCtWfpb7AX4RBYUvWXwGd1_aFAEjoyODVmMv0syOjNFoHSy0g6j5uU7Jes&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnZMTxbiq-6Rk6w5wajajLa3eSApBkTHioMobQ54DBz_cnQliOe3OXYc_5dQof7qLZn3Q&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT4hrnTXz5Dr4aVHN6xkb3hg85Q5F4z5Nxiboi8o176skOSZjlTHh99NkaDt8e-SqznwCs&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxpXTxE5QO4Y5S6FHQYiQ6-uf1Qwe6FHb28YDggeuamrPOUsIdP2Nt1OlY6sCZgJSYFI&usqp=CAU\")\n",
        "predict_steak_or_pizza(url=\"https://media-cdn.tripadvisor.com/media/photo-s/17/98/96/31/photo0jpg.jpg\")\n",
        "predict_steak_or_pizza(url=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTyo8-Fg7Pa_8RQdKuLrWY6A5MQDIQgQxPuVTzA4Po8On3rMWl9I9NOY24WLIpHMOqUyss&usqp=CAU\")"
      ],
      "metadata": {
        "id": "LlOspLy2Nphl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we got a 4/2. It's better! With a simple data augmentation, we increase the quality of our model."
      ],
      "metadata": {
        "id": "r9YDozAuYBPh"
      }
    }
  ]
}